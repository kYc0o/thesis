\chapter{System facilities for a models@runtime implementation}
As we discussed on the previous chapters, the presented middleware is able to represent a running system in the form of a model@runtime, according to the Kevoree meta-model.
Indeed, this representation can be manipulated by the existing Kevoree editor, on which we can modify parameters, add or remove nodes and components or bind component's ports using channels.
However, in order to realize these modifications, system facilities should be leveraged, such as component instantiation, deploy unit downloading, and dynamic loading of binaries for new deployments.
Moreover, means to store a serialized model and easy access to it must be present on the system.

In this chapter, we discuss the current requirements from the underlying system, as well as the hardware device features (\textit{i.e.} connectivity, storage) needed to achieve a dynamic behavior.
Afterwards, some technical contributions to an IoT operating system will be presented, since the complexity of our middleware leads to specific OS requirements that are not often provided.

\section{Kevoree-IoT operating system requirements}
Our middleware approach will need several features from the underlying system, in order to match with the high-level description provided by the model@runtime.
As we discussed on our state of the art in Section \ref{sec:IoTDeployment}, the most common approach used to run applications on IoT devices is bare-metal development followed by firmware flashing into the ROM memory.
Even if this method allows a fine control of the underlying hardware, the development time can be very long and difficult to debug, since abstractions are mostly done only at the hardware level, and does not come to the system level.
Moreover, since complexity grows, applications for IoT should be developed without special attention to hardware and system concerns.
Thus, an IoT operating system should be used, in order to leverage its system-level abstractions.

As presented in the state of the art, several IoT operating systems exist, but according to the needed features some of them are more convenient.
The most important features are listed as follows:
\begin{enumerate}
	\item \textbf{Network stack implementing basic IP functions (TCP, UDP, HTTP, CoAP).} In order to share a model and download software artifacts, IP communication is mandatory, since the goal is to use the Internet to reach component repositories from different sources.
	\item \textbf{Persistent data handling, preferably a file system.} The method used to avoid RAM storage is to serialize the model@runtime in a file on the flash memory, usually in the JSON format. Thus, a way to store and access this JSON file for reading and writing should be provided.
	\item \textbf{Dynamic linker and loader, following the third approach presented in section \ref{sec:IoTDeployment}.} A dynamic linker and loader is essential to our approach, since changes in the model containing new software components will trigger the download and instantiation of an artifact, which should be linked and loaded at runtime by the underlying system.
	\item \textbf{Abstractions for attached devices (LEDs, sensors, actuators, etc.).} Although not mandatory, an OS usually carry basic hardware abstractions, providing an easy way to develop applications which need a physical interaction with the external world.
	\item \textbf{Basic OS functionalities such as timers, task scheduler and Inter Process Communication (IPC).} Basic functionalities that are the core of any embedded OS.
\end{enumerate}
In order to make a rapid functional prototype of our middleware, we will make use of the Contiki OS which, given the features presented above, seems to fit our minimal requirements.
Despite the programming model already described as a drawback, Contiki offers all the needed functionalities, as well as a wide community which collaborate very actively in the development and debug of it.
%Therefore, we use the ContikiOS\cite{dunkels2004contiki} in order to provide an efficient way to distribute components, since it allows dynamic loading of binary modules.
Moreover, Contiki includes an implementation of 6loWPAN\cite{rfc4944}, an adaptation layer for an IPv6 compressed stack, which let us assign directly an IPv6 address to the device.
This enables a ready to use IoT environment.
Afterwards, an UDP transport layer is provided, allowing a standard way to reach UDP servers to download the needed deploy units to perform system adaptations, according to the model@runtime engine.
%We use erbium's\cite{rfc7252} CoAP implementation in order to have a REST engine which is used as a main communication channel between nodes. 
Indeed, one of the most important features provided by this OS is the dynamic code loading mechanisms.
These are based on a dynamic linker and loader that use the standard ELF object file format\cite{dunkels06runtime}.

However, even if the OS fulfill all the requirements, the computational resources needed by our middleware should be taken into account, in order to use a flexible hardware platform on which we can evaluate our approach.
As presented previously on chapter \ref{ch:MARContiki}, the minimum requirements for our first implementation have already been defined, resulting in the design of a new board, followed by the tests on the IoT-Lab M3 platform.
Indeed, testing on a large scale testbed facilitates the firmware flashing, serial output for debugging and power monitoring, thanks to the tools provided by IoT-Lab\footnote{\url{https://www.iot-lab.info/tools/}}.
Moreover, a Contiki port is maintained and supported by the IoT-Lab team, providing a ready to use environment for our already developed tools.
% as we already introduced on section \ref{sec:iotlab}
However, two important features lacked in this port: the low-level interface for the Contiki File System (CFS) and the relocation operations needed by the Contiki ELF loader.
Therefore, given the importance of these features for the correct implementation of our middleware, it was necessary to extend the testbed capabilities to add support for the CFS and the ELF loader.
The next subsection will give details about the technical contributions provided for the IoT-Lab testbed, in order to have a complete IoT platform able to run our middleware and ready for our large-scale tests.

\subsection{Extending the FIT IoT-Lab testbed}
As stated previously, two missing features were detected in the Contiki port made for this testbed.
The first one was the implementation of a low-level interface necessary for the Contiki File System.
This driver make use of low-level functions to read, write and erase the flash memory embedded on the M3 node.
Therefore, this functionality is necessary to leverage the high-level abstractions provided by the CFS, which are a very useful approach to manipulate files into the flash memory, also needed by the ELF loader.

%\todo{Until here it's ok, the rest of the chapter needs to be reworked}

\subsubsection{File system driver implementation}
The definitions and porting instructions for CFS can be found on the wiki page of the Contiki main repository\footnote{\url{https://github.com/contiki-os/contiki.git}} as well as in the article introducing this file system in \cite{tsiftes09enabling}.
A brief description of the file system is given as follows:

\begin{citeverbatim}
	Contiki provides a set of file systems for using various kinds of storage devices in resource-constrained systems. 
	All of these file systems implement a subset of the Contiki File System (CFS) interface, and two of them provide the full functionality: CFS-POSIX and Coffee.
	CFS-POSIX is used in Contiki platforms that run in native mode. 
	It uses direct calls to the POSIX file API that is provided by the host operating system. 
	Coffee, on the other hand, is primarily aimed at sensor devices that are equipped with flash memories or EEPROM.
\end{citeverbatim}

Thus, the usage of the \textit{Coffee} API is the one that should be implemented for the M3 platform.
To do so, information about the flash memory embedded on the platform must be mapped into the configuration headers used by Contiki, in order to provide a connection between the physical device and the OS.
This information consists in several parameters, such as:
\begin{description}
	\item[Total size.] The actual size of the memory flash.
	\item[Sector size.] A flash memory is divided by sectors. This information allows the CFS to organize the way the memory will be written.
	\item[Page size.] This parameter is given by the manufacturer of the flash memory and allows to organize writing cycles in a more efficient manner, avoiding fragmentation.
	\item[Coffee start.] It describes the actual memory address where Coffee can start to write. It allows to reserve a memory space that cannot be accessible from the API, providing a secure space where we can store read-only data (\textit{i.e.} initialization data, node ID, etc.).
	\item[Coffee size.] This is the overall memory available for reading/writing using Coffee. It is calculated by the difference between the total size and the Coffee start size.
	\item[Coffee name length.] With a view on internal memory savings, a limitation on file names is imposed. This will avoid the use of large filenames which have an impact on the memory footprint of the CFS.
	\item[Coffee max. open files] In the same way as name length, the number of simultaneous open files can increase the memory usage by the file system. Thus, a limitation on this concern is needed.
\end{description}

Afterwards, parameters to configure the internal behavior of Coffee are needed, in order to provide more or less flexibility on the usage.
Those parameters are set on the maximum optimal levels for our platform, in order to give the maximum flexibility as possible.

%As we can deduce, we aim to port the Coffee file system, since our platform is a resource constrained device embedding a flash memory.
%Thus, the porting instructions state that the first step is to provide information about the external memory.
%This is done through the \texttt{cfs-coffee-arch.h} file, which in our case was created as shown in listing \ref{lst:flashIface}
%\lstset{language=C,
%	basicstyle=\ttfamily\scriptsize,
%	keywordstyle=\color{blue}\ttfamily,
%	stringstyle=\color{red}\ttfamily,
%	commentstyle=\color{gray}\ttfamily,
%	breaklines=true,
%	captionpos=b
%}
%\begin{lstlisting}[language=C, caption=Contiki header for external memory features, label=lst:flashIface]
%/*** N25Q128 Memory Organization
%The memory is organized as:
%16,777,216 bytes (8 bits each)
%256 sectors (64 Kbytes each)
%In Bottom and Top versions: 8 bottom (top) 64 Kbytes boot sectors with 16 subsectors
%(4 Kbytes) and 248 standard 64 KB sectors
%65,536 pages (256 bytes each)
%64 OTP bytes located outside the main memory array
%Each page can be individually programmed (bits are programmed from 1 to 0).
%The device is Sector or Bulk Erasable (bits are erased from 0 to 1) but not Page Erasable.
%Subsector Erase is allowed on the 8 boot sectors (for devices with bottom or top architecture).

%*/
%//Total size of the External Flash Memory in the M3 node
%#define COFFEE_XMEM_TOTAL_SIZE_KB       16384UL

%/* Coffee configuration parameters. */
%#define COFFEE_SECTOR_SIZE   	65536UL
%#define COFFEE_PAGE_SIZE       	256UL
%#define COFFEE_START            0
%#define COFFEE_SIZE             (COFFEE_XMEM_TOTAL_SIZE_KB * 1024UL - COFFEE_START)
%#define COFFEE_NAME_LENGTH      32
%#define COFFEE_MAX_OPEN_FILES   6
%#define COFFEE_FD_SET_SIZE      8
%#define COFFEE_LOG_TABLE_LIMIT 	256
%#define COFFEE_DYN_SIZE         16*1024
%#define COFFEE_LOG_SIZE         8*1024

%#define COFFEE_MICRO_LOGS       1

%/* Flash operations. */
%#define COFFEE_WRITE(buf, size, offset)                \
%xmem_pwrite((char *)(buf), (size), COFFEE_START + (offset))

%#define COFFEE_READ(buf, size, offset)                \
%xmem_pread((char *)(buf), (size), COFFEE_START + (offset))

%#define COFFEE_ERASE(sector)                    \
%xmem_erase(COFFEE_SECTOR_SIZE, COFFEE_START + (sector) * COFFEE_SECTOR_SIZE)
%\end{lstlisting}

Once the flash parameters are given, some functions allowing the actual read/write/erase onto the flash memory must be also mapped in the interface.
Thus, functions from the lower driver implementation are needed.
Indeed, these are provided by the Hardware Abstraction Layer, already included in the IoT-Lab low-level driver implementation.
This HAL is included in a separated repository called \textit{openlab}\footnote{\url{https://github.com/iot-lab/openlab.git}}.
Within this repository, all the low-level drivers providing functions to manipulate every piece of the embedded system are included.
All the functions related to the flash memory usage are available in detail in annex X.
%Our concern is to identify those related to the SPI Flash memory.
%Indeed, several functions to make use of the flash memory were available, including:
%\begin{enumerate}
%	\item \texttt{\textbf{void n25xxx\_read\_id(uint8\_t *id, uint16\_t len):}} Reads the flash chip ID which contains the manufacturer ID, the device ID and an unique ID.
%	\item \texttt{\textbf{void n25xxx\_read(uint32\_t address, uint8\_t *buf, uint16\_t len):}} Reads a given amount of data from the flash starting anywhere in the flash.
%	\item \texttt{\textbf{n25xxx\_write\_enable()}} and \texttt{\textbf{n25xxx\_write\_disable():}} Enable/Disable writing on the flash.
%	This function must be called before/after each write operation.
%	\item \texttt{\textbf{void n25xxx\_write\_page(uint32\_t address, uint8\_t *buf):}} This function writes the content of a buffer to a given flash page.
%	The write must be enabled before calling this function.
%	\item \texttt{\textbf{void n25xxx\_erase\_subsector(uint32\_t address):}} Erases a given sub-sector of the flash, a sub-sector is composed of 16 pages (i.e. 4096 bytes).
%	The write must be enabled before calling this function.
%	\item \texttt{\textbf{void n25xxx\_erase\_sector(uint32\_t address):}} Erases a given sector of the flash, a sector is composed of 16 sub-sectors (i.e 256 page or 65536 bytes).
%	The write must be enabled before calling this function.
%	\item \texttt{\textbf{void n25xxx\_bulk\_erase():}} This function erases the entire flash.
%	The write must be enabled before calling this function.
%	\item \texttt{\textbf{uint8\_t n25xxx\_read\_status(void):}} Read the flash status register.
	
%	Additionally, three more functions were added to this HAL in order to have full control over the flash memory, needed by the Contiki low-level interface.
	
%	\item \texttt{\textbf{void \_n25xxx\_cs\_clear(void)}} and \texttt{\textbf{void \_n25xxx\_cs\_set(void):}} Chip select clear/set allowing to turn on/off an SPI device.
%	\item \texttt{\textbf{uint8\_t \_n25xxx\_rw\_byte(uint8\_t byte):}} Allows to read only one byte at once.
%	The CS must be set/cleared after each use of this function.
%\end{enumerate}
%With the use of this functions, the next step was to map them to the Contiki's needed interface.
%The full code of the Contiki's wrap functions mapped to the openlab's platform functions is available at the main repository\footnote{\url{https://github.com/iot-lab/contiki/blob/master/platform/openlab/dev/xmem.c}} of the Contiki port for IoT-Lab. \todo{should I use an annex instead?}

Once the porting of the file system is done, we can take care about the ELF loader.
This second technical contribution aims to provide the relocation functions for the embedded ARM Cortex-M3 CPU, used by the Contiki ELF loader in order to find the correct addresses of the symbols provided by the OS.
Indeed, the ELF loader is the main feature used to add new modules to the kernel or update its existing features.

\subsubsection{Runtime address relocation for ARM Cortex-M3 platforms}
As presented in section \ref{sec:IoTDeployment}, we have studied the different methods to add new modules into a running IoT device.
The method used by the Contiki ELF loader is the relocatable code.
Indeed, this approach needs to relocate the temporary addresses given to the unresolved symbols of a new module, in order to get access to the needed functions provided by the OS.

Since the use of the Contiki ELF loader involves two aspects, a prepared firmware able to load new modules and the new modules themselves, modifications to the compilation routines should be provided for both artifacts.
As described in the Contiki wiki\footnote{\url{https://github.com/contiki-os/contiki/wiki/The-dynamic-loader\#Preparing_a_Firmware_for_ELF_Loading}}, a firmware able to load new modules should be prepared as follows:
\begin{citeverbatim}
	The firmware must be prepared with a symbol table to able to load ELF modules dynamically.
	This three-step process ensures that all available symbols in the firmware are also visible in the symbol table, along with a pointer to their address.
	
	\texttt{make <firmware-name>} \\
	\texttt{make CORE=<firmware-name> <firmware-name>} \\
	\texttt{make CORE=<firmware-name> <firmware-name>} \\
\end{citeverbatim}
Thus, compilation instructions for the \texttt{CORE} variable must be provided.
Since they are CPU specific, they must be added in the Makefile for our specific platform.
\\
\lstset{language=make,
	basicstyle=\ttfamily\scriptsize,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{gray}\ttfamily,
	breaklines=true,
	captionpos=b
}
\begin{lstlisting}[language=make, caption=Compilation settings to create a proper symbol table, label=lst:make4Symbols]
ifdef CORE
.PHONY: symbols.c symbols.h
symbols.c:
$(NM) $(CORE) | awk -f $(CONTIKI)/tools/mknmlist > symbols.c
else
symbols.c symbols.h:
cp ${CONTIKI}/tools/empty-symbols.c symbols.c
cp ${CONTIKI}/tools/empty-symbols.h symbols.h
endif
\end{lstlisting}

Listing \ref{lst:make4Symbols} shows the instructions to create a symbol table for the M3 platform, including all the functions used in the base firmware.
These symbols are created using the \texttt{arm-none-eabi-nm} command to extract all the symbol's names from the main firmware, and with the aid of a Contiki tool called \texttt{mknmlist} a source C file is generated where all used symbols are declared.
Finally, this source code is compiled and linked to the main firmware.
On the other hand, if the \texttt{CORE} variable is not set, a predefined source file without the symbols is copied to be compiled with the main firmware.

As for the creation of new ELF modules, the Contiki dynamic linker and loader\cite{dunkels06runtime} depends strongly on the relocation methods supported by the CPU architectures for which the module is being compiled.
Since our implementation is based on an ARM architecture, we need to provide the relocation functions for this platform.
Indeed, ARM offers the processor-specific definitions in the \textit{ELF for the Application Binary Interface (ABI) for the ARM architecture}\footnote{\url{http://infocenter.arm.com/help/topic/com.arm.doc.ihi0044e/IHI0044E_aaelf.pdf}} document.
This document specifies the way of an ELF binary should be produced by a compiler, including the relocation types.
Thus, this information should be used to implement a dynamic linker which will be in charge of the relocation process for unresolved symbols present in the new module.
For the ARM architecture, more than 100 relocation types exist, but just a few operations still relevant.
We will focus in only 3 types of relocation, which were present in most of the examples we compiled.
These relocation types are presented in table \ref{tab:relocTypes}.

\begin{table}[htb]
	\centering
	\caption{Relocation types compatible with our loader}
	\label{tab:relocTypes}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Code} & \textbf{Name}       & \textbf{Type} & \textbf{Class} & \textbf{Operation} \\ \hline
		2             & R\_ARM\_ABS32       & Static        & Data           & (S + A) | T        \\ \hline
		10            & R\_ARM\_THM\_CALL   & Static        & Thumb32        & ((S + A) | T) – P  \\ \hline
		30            & R\_ARM\_THM\_JUMP24 & Static        & Thumb32        & ((S + A) | T) – P  \\ \hline
	\end{tabular}
\end{table}

These three relocation types are the most commonly found in small component examples, although 10 and 30 share the same operation.
Indeed, these binary operations are the most important part of our implementation, and were coded on our platform-specific driver for Contiki.

%We can appreciate that type 10 and 30 have the same operation.
%The following nomenclature is used for the operation:
%\begin{itemize}
%	\item \textbf{S} (when used on its own) is the address of the symbol.
%	\item \textbf{A} is the addend for the relocation.
%	\item \textbf{P} is the address of the place being relocated (derived from r\_offset).
%	\item \textbf{T} is 1 if the target symbol S has type STT\_FUNC and the symbol addresses a Thumb instruction; it is 0 otherwise.
%\end{itemize}

%Therefore, we need to implement only 2 types of relocations.
%These two types of relocation depends on the information provided by a compatible ELF file, which is first parsed by the Contiki ELF loader.

Afterwards, the requirements of an ELF module handled by the Contiki ELF loader are stated on the Contiki's wiki, as following:

\begin{citeverbatim}
	An ELF file consists of a header followed by a set of sections which typically include at least a section for binary code (\texttt{.text}), a section for statically allocated data with pre-assigned values (\texttt{.data}), and a section for zero-initialized data (\texttt{.bss}).
	Additionally, each symbol is represented in a symbol table (\texttt{.symtab}), and strings are stored in a string table (\texttt{.strtab}). 
	For a file to be accepted by Contiki's ELF loader, it must contain at least the sections listed above. 
\end{citeverbatim}

In order to produce an ELF file compatible with this characteristics, we need to provide the compilation recipes following the method specified by Contiki.
Indeed, the Contiki's documentation provide a generic method to produce a Contiki ELF module (CE), as stated below:

\begin{citeverbatim}
	Contiki's build system includes an easy method to create modules that can be loaded into a running Contiki system.
	Simply compile the program with the suffix .ce, as shown below. 
	The suffix instructs the make program to compile an ELF file from a C source file. 
	The object module is stripped from unneeded symbols. 
	The .co suffix works similarly, but does keeps unneeded symbols in the ELF file. 
	
	\texttt{cd example/hello-world} \\
	\texttt{make TARGET=sky hello-world.ce}
\end{citeverbatim}

This method makes use of Contiki's predefined compiler flags that should produce a compatible ELF file.
However, such flags are intended mostly for MSP430 architectures.
We have experimented these default flags for the CE compilation and it was found that several other sections were present in the ELF file.
These "extra" sections contained needed information about the module, and were skipped by the ELF loader.
Thus, specific compilation mechanisms shown on listing \ref{lst:make4CELF} have been proposed to solve this problem.
\\
\lstset{language=make,
	basicstyle=\ttfamily\scriptsize,
	keywordstyle=\color{blue}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	commentstyle=\color{gray}\ttfamily,
	breaklines=true,
	captionpos=b
}
\begin{lstlisting}[language=make, caption=Compilation instructions to generate an ARM ELF file., label=lst:make4CELF]
CUSTOM_RULE_C_TO_CE = "defined"
%.ce: %.c
@# Requires '-DAUTOSTART_ENABLE' to be loaded
$(CC) $(CFLAGS) $(OPENLAB_INCLUDE_PATH) -DAUTOSTART_ENABLE -c $< -o $*.o
$(GCCPREFIX)-ld -r -T $(OPENLAB)/merge-segments.ld $*.o -o $@
$(STRIP) --strip-unneeded -g -x $@
\end{lstlisting}

We can highlight that a special linking phase (triggered by the \texttt{-ld} flag) using a linker script was needed to merge the extra sections generated by the standard compilation phase, followed by the stripping mechanism for unneeded symbols (mostly debug symbols).
This script consists in merge all \texttt{.text, .data, .rodata} and \texttt{.bss} related sections into common, unified ones.
This is needed for the ELF parser provided by Contiki, which is intended to only parse the sections mentioned above.

%For instance, without this script, a hello world example generated the following sections:

%\begin{listing}
%	\texttt{.text.process\_thread\_hello\_world\_process} 
%	\texttt{.rel.text.process\_thread\_hello\_world\_process}
%	\texttt{.rel.data.hello\_world\_process}
%	\texttt{.rodata.autostart\_processes}
%	\texttt{.rel.rodata.autostart\_processes}
%	\texttt{.rodata.str1.1}
%\end{listing}

%This are the standard sections generated by the \texttt{arm-none-eabi-gcc} compiler without the linking phase where the script is executed.
%Since sections other than the required by the loader are ignored, the loading attempt will result in an unsuccessful module execution.
%Details about the script are shown in listing \ref{lst:script4CELF}.

Once the correctness of the ELF file is validated by the loader, a relocation phase is conducted.
This makes use of the ELF loader internals, which should be defined by the platform.
These steps are described by Contiki as follows:
\begin{citeverbatim}
	Each CPU architecture in Contiki that supports ELF loading implements a set of architecture-dependent functions. 
	The table \ref{tab:loaderFunc} shows the API that needs to be implemented in order to support ELF loading. 
	These include functions allocate RAM for data (\texttt{elfloader\_arch\_allocate\_ram()}), allocate read-only memory (ROM) for code (\texttt{elfloader\_arch\_allocate\_rom()}), write to the allocated ROM (\texttt{elfloader\_arch\_write\_rom()}), and relocate addresses in the ROM (\texttt{elfloader\_arch\_relocate()}). 
	The relocation information is stored in objects of type \texttt{struct elf32\_rela}, which is defined below in listing \ref{lst:ELFreloc}. 
	In this structure, \texttt{r\_offset} indicates the location to be relocated, \texttt{r\_info} specifies the relocation type and symbol index, and \texttt{r\_addend} is the value to add when relocating an address in \texttt{elfloader\_arch\_relocate()}.
\end{citeverbatim}

\begin{table}[htb]
	\centering
	\scriptsize
	\caption{The architecture-dependent functions in the ELF loader}
	\label{tab:loaderFunc}
	\begin{tabular}{p{9cm}l}
		elfloader\_arch\_allocate\_ram(int size)                                                                                        & Allocate RAM              \\
		elfloader\_arch\_allocate\_rom(int size)                                                                                        & Allocate ROM              \\
		elfloader\_arch\_write\_rom(int fd, unsigned short textoff, unsigned int size, char *mem)                                       & Program ROM.              \\
		elfloader\_arch\_allocate\_relocate(int fd, unsigned sectionoffset, char *sectionaddress, struct elf32\_rela *rela, char *addr) & Relocate addresses in ROM
	\end{tabular}
\end{table}

\begin{lstlisting}[language=C, caption=The 32-bit ELF relocation structure, label=lst:ELFreloc]
struct elf32_rela {
elf32_addr	r_offset;
elf32_word	r_info;
elf32_sword	r_addend;
};
\end{lstlisting}

These architecture-dependent functions were implemented for the common openlab platform.
Moreover, for the allocation ROM and RAM functions, a pre-allocated space of both was necessary, which was declared in the configuration header for the IoT-Lab M3 platform.
The source code is available in the master branch of the Contiki port for the IoT-Lab testbed.

Finally, once all symbols are located in the right place and the code is copied into its respective memory space, an entry point to the new module is provided as a new Contiki process.
Indeed, this new feature is available on the list of processes, and is now possible to access it by using events.
The Contiki message passing mechanisms are used to access the new provided functionalities, if needed.
In a common module loading behavior, the new process will start automatically as if it was embedded from the beginning.

\section{Summary}
The Contiki OS provides most of the functionalities that are mandatory for the good implementation of our models@runtime middleware.
As we described in this chapter, two crucial functionalities were not implemented on the IoT-Lab M3 platform: the File System and the ELF Loader.
Indeed, without these functionalities the adaptations generated by the model@runtime engine could not be executed.
Since the goal of our middleware is to deploy new modules by the means of an ELF file, these contributions are mandatory.
Naturally, easy storage and access for this file simplifies the task of loading, by leveraging the abstractions proposed by the Contiki File System.
Moreover, by adding the relocation and dynamic linking mechanisms to the M3 platform, real deployment of new features is now possible.

The added support for the two main functionalities needed by our middlware was crucial for the continuation of our research, since it allowed to conduct the evaluation on real nodes.
Without them, our experimentations would have stopped at the model level, without enacting the actual changes represented in the model at the system level.
The relevancy of our implementations was acknowledged by the FIT IoT-Lab maintainers, who accepted our pull-request\footnote{\url{https://github.com/iot-lab/contiki/pull/2}} including this technical contributions in their main fork of the Contiki repository.

In order to continue our research goals, we need to review our results and the tools already provided by our middleware.
Indeed, several challenges were raised with the results of our first model@runtime implementation, which is now completed thanks to the system facilities described on this chapter.
%The first one, lies in the way that the model should be spread across the network.
%Since the same model must be present in all the nodes forming the network, we must investigate epidemic algorithms of distribution, such as Deluge\cite{hui2004dynamic}.
One of these challenges is about the creation of components and their distribution across the network.
Indeed, special attention should be put on this mechanism, since we have argued throughout this thesis that most of the energy consumption is due to radio communication, on battery powered wireless nodes.
Therefore, distributing software components on mesh topologies, which are less robust than typical computer networks, raise scientific questions about the best method to distribute them.
Thus, a distributed algorithm to download components taking into account the network topology and energy consumption should be proposed.
The next chapter will discuss how these components can be built, and how they can be distributed in an energy-efficient manner.
