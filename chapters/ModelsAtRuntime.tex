\chapter{Managing software deployment in distributed systems}
\label{sec:managingDS}
As we discussed in the previous chapter, we can compare classical distributed systems with IoT systems.
Since they share both the high number of participating nodes and the collaborative approach to offer services, the existing proposed solutions to manage the software layer in classical distributed systems are of high interest, and provide a good insight for their adoption in the IoT.
A definition of a distributed system proposed by Coulouris \textit{et al.} can be found at \cite{coulouris2005distributed}:
\begin{citeverbatim}
	" We define a distributed system as one in which hardware or software components located at networked computers communicate and coordinate their actions only by passing messages."
\end{citeverbatim}
With this definition, we can clearly realize an IoT system that fit this characteristics, being an IoT device a computing machine which is part of a network including other IoT devices.
Thus, the challenges arose by classical distributed systems will be the same for the IoT based systems.
Taking this into account, it is possible to explore existing solutions that are most adapted to IoT systems, regarding the main differences stated in the synthesis of the previous chapter.

First, the high acceptance of Component Based Software Engineering (CBSE)\cite{crnkovic2002building} approaches to deal with the large size of a software architecture lead our investigations to explore it for its potential use in the IoT, in order to reliably manage the life cycle of these large information systems.

Second, as the system evolves, new dynamic deployments will be required to offer adapted functionalities.
This is challenging in single software deployments, and even more in a distributed environment.
Kramer and Magee\cite{kramer1990evolving} provided, albeit in an informal way, in their article that introduces quiescence, a definition for what we consider to be dynamic evolution:
\begin{citeverbatim}
	" [Evolutionary change] may involve modifications or extensions to the system which were not envisaged at design time.
	Furthermore, in many application domains there is a requirement that the system accommodate such change dynamically, without stopping or disturbing the operation of those parts of the system unaffected by the change. "
\end{citeverbatim}
This kind of evolutions will be systematically present in IoT systems, as we stated for instance in the case of building automation in section \ref{subsec:effBuilding}.

This two concepts are essential to build an IoT software architecture.
Thus, the need of an unified tool managing this two aspects of the system comes with the design of such architecture.
However, at this point the main differences regarding IoT systems and classical distributed system become more relevant, since the already investigated approaches are intended for the latter.
Taking this state of the art as a source of inspiration, the purpose of this thesis is to propose a novel way to manage software deployment in IoT systems, facing at the same time with the common issues of distributed systems and the new ones raised by the new IoT infrastructure.

\section{Challenges in distributed environments}
Distributed systems cannot be managed as centralized systems or single desktop machines.
This is due to the very different application domain in which a distributed approach is needed.
While desktop applications can be easily deployed through local package and update managers (either at application or OS levels), distributed applications cannot be deployed nor updated using the same methods.
Let's take the example of a set of desktop machines running a typical OS, in which basic functionalities are the same, as well as provided services.
When a new feature is released for such OS, a single binary including the new feature is deployed in all the machines running the OS, regardless of the underlying hardware.
On the other hand, a distributed application cannot be spread among all nodes within the same binary, since each one can offer a different service, or can be in charge of only a part of a bigger system, being the node's hardware capabilities not enough to run such a big application.
This is true for most of the distributed applications, since the goal is to share the resources of several machines (servers) to provide more and better services.
Thus, the software complexity can be divided among several machines, deploying different parts of the application in different nodes.
A big challenge appears when it is necessary to deploy new features or update the current ones in this distributed environments, since it is not possible to add or update this features for each machine: first due to the quantity and often the physical location of the equipment and second to the continuous demand of the application, which cannot be stopped.
The need of a fine management of this software layer is then justified.


Another issue comes with the networking layer availability, which is mandatory for distributed applications.
Since communication is the main activity in a distributed environment, network robustness is a must.
However, it is complicated to estimate the network usage for a given application.
As an example, web applications are often exposed to this problem, since they offer their services to an undetermined number of clients.
It is known that as the client requests increase, the application is more susceptible to crash, due to the complex network management of all client's connections.
Usually this problem is solved deploying more servers in order to increase the number of maximum connections, increasing also the cost of the infrastructure.
In IoT environments this is more complicated, since the number of \textit{things} that are used as servers is limited by the architecture itself, thus the deployment of new things is more complicated.
Moreover, the network robustness found in classical distributed systems is not the same as in IoT systems.
This is due to the network topology already described in the previous chapter.
Even if new protocols to improve this robustness are being developed\cite{thubert2013ietf}, there is always a trade-off between network availability and energy consumption, since most of the communications are done wirelessly by battery powered nodes.
Advancements in more energy efficient transceivers and protocols could improve the communication facilities, giving place to a more flexible usage of the network.

The difficulties presented in this section must be taken into account while developing a solution charged of the distributed applications' deployment.
Thus, deployment should be defined clearly in order to establish the main functionalities provided by the proposed solutions, which should overcome such difficulties.

\section{Dynamic deployment in distributed systems}
Several definitions of deployment can exist, since the perspectives of software life cycle can be different.
For this thesis, I will give my own definition of software deployment, which covers from the availability of a binary file ready to be executed, to the distribution, dynamic loading and linking on a running platform, either as a new feature or to perform an update.

An \textit{eternal system} refers to a system which services are required in a constant manner.
Thus, stopping it to make changes such as updates, add new features or other improvements is not allowed.
Such services may include life-critical systems, financial systems, telecommunications, and air traffic control, among many others, including IoT systems which are our concern.
Therefore, techniques are needed to change software while it is running. 
This is a very challenging problem and is known under a variety of terms, such as, but not limited to, runtime evolution, runtime reconfiguration, dynamic adaptation, dynamic upgrading, hot updating, dynamic evolution and so on, sharing the common issue of dynamic deployment.
%In this thesis, we will focus on adapting an IoT architecture and apply those changes at runtime, providing dynamic adaptation through dynamic deployment.

The deployment process usually starts when code is written, or generated, in a programming language, then compiled into binary code to be executed.
Each module of the application is then produced in a form of object file, which a linker can then use to construct a final executable binary or a library (e.g., .dll, .so, .a) if desired.
A symbol table is also embedded with information that defines its dependencies (e.g., shared libraries).
When this code is executed, a process of dynamic linking takes place.
This step is quite different from the previous one, taking into account the information of shared libraries included in the executable file, in order to bind them dynamically to the running process.
A dynamic loader should be provided with the OS, being different one from each other, providing such functionality.
The same step is performed while updating such a process, with the difference that the previous process must be stopped to be replaced by the new one.
Another approach is proposed for interpreted languages, in which the compilation phase does not take place.
The code in this case is directly executed through an interpreter, which can also use an hybrid approach mixing compilation and interpretation (i.e. Java and .NET).
In this case, the source code is compiled to an intermediate bytecode format, which can be interpreted by a Virtual Machine (VM).
Some optimizations are done in the case of Java, where classes are loaded only when needed.

Thus far, the proposed deployment concept is explained, which can be represented in figure X \todo{make figure of the deployment cycle}, including the creation, distribution and maintenance of a given application.
This process can be valid for any application, for both classical and IoT systems.
However, there is no a clear nor standard way to develop applications for devices being part of the IoT.
As of classical distributed systems, the development model is based on creating applications to be deployed in specific operating systems.
Granted that the World Wide Web is clearly the biggest distributed system, we will take into account the most popular OS used as a platform for web based applications, in order to find the most common deployment environment.
This survey \cite{servers2015} shows that most of the servers offering websites hosting are UNIX based, being Windows in a second place.
With this results, we can state that very often a distributed application will be deployed either in a UNIX or Windows server.
Package managers and update engines are the most common methods to provide new functionalities and OS updates, while JVMs based applications utilize a different mechanism of distribution.


The process of software deployment can differ sightly from classical distributed systems to IoT systems.
This is due to the underlying hardware differences discussed throughout this state of the art.
While non-constrained nodes present in classical distributed systems are able to run VMs to execute precompiled bytecode, written in high level languages such as Java, IoT devices are not able to run a complete JVM, but rather very limited ones\cite{levis2002mate}\cite{brouwers2009darjeeling}.
Moreover, related research\cite{oliver2014reprogramming} found JVMs very resource consuming in small devices typical of IoT, since they add a considerable CPU overhead while running, which avoids a long-term use for battery powered nodes.
Thus, we will focus in the software deployment for applications compiled as binary code, which can be executed directly by the native platform.
Indeed, in the embedded systems domain, which embodies most of the devices used in IoT, bare metal applications are the most common procedure to provide services or functionalities, being physical flashing of the binary image the usual deployment method, since it can be done at the moment of the embedded system assembly, without considering any further firmware changes.
With this limitation, it is even more complicated to distribute applications among several nodes which are physically separated, since the physical flashing of every node in a typical IoT system cannot be worth considering.
Thus, a choice of the current alternatives to deal with this problem should be made.

Operating systems used in embedded systems provide several methods to deploy new applications at runtime, being the most common:
\begin{enumerate}
	\item Scripting languages \cite{dunkels2006low}\cite{kovatsch2012actinium}.
	\item Virtual Machines \cite{levis2002mate}\cite{brouwers2009darjeeling}.
	\item Kernel replacement \cite{hui2004dynamic}.
	\item Position Independent Code \cite{han2005dynamic} \cite{chen2010enix}.
	\item Relocatable code \cite{david2005wildcat}\cite{dong2009dynamic}\cite{dunkels2004contiki}.
\end{enumerate}
As we discussed in this section, research shows that only non-interpreted code is worth considering for long term applications, thus scripting languages and VM approaches will be discarded.

\subsection{Dynamic deployment in the IoT}
As presented in this section, applications deployment for IoT systems differs in several ways from classical distributed systems.
In order to cope with this differences, which are mostly hardware and network related, approaches coming from the embedded systems and Wireless Sensor Networks (WSN) domains have been proposed.
We will discuss three methods already introduced above: 
\begin{itemize}
	\item \textbf{Kernel replacement.} This is a straightforward solution to deploy new features or bug fixes in embedded systems, an essential part of the IoT infrastructure presented in this thesis.
	It consists in change the entire kernel image for a new one, which must be either flashed physically or transmitted through the network, followed by a complete reboot of the system.
	Indeed, sharing a large kernel image can be very energy consuming for wireless battery powered devices.
	Advantages of this technique are the possibility of deep changes into the kernel or applications, as well as deployment of a complete different OS.
	\item \textbf{Position Independent Code (PIC).} As mentioned before, the executable code usually follows the process of dynamic relocation to find the needed symbol's addresses, then execute the code. 
	PIC follow a different approach to run applications. 
	It consists in the use of relative jumps rather than absolute addresses to find symbols, mitigating the need of run-time relocation.
	Several overheads can be introduced by the use of this technique, such as pre-loading of addresses before making jumps, more needed jumps while calling kernel or another module, functions registration and de-registration and the necessity of a jump table.
	Furthermore, a PIC compatible compiler should be used, in order to produce a loadable module using this method, which can be unavailable for certain CPU architectures.
	For instance, AVR microcontrollers supports this type of compilation, but limited to a 4KB program size, while it is not known a compiler supporting PIC for MSP430 CPUs.
	Applications running non-PIC methods to deploy new modules have shown a 13\% better performance compared to PIC\cite{dong2009dynamic}.
	However, the global efficiency while running most of PIC modules is the same as if they were flashed directly on the device.
	\item \textbf{Relocatable code.} This technique is the one used by classical OSs such as Unix based and Windows, also implemented for IoT devices\cite{dunkels06runtime}. 
	It uses run-time dynamic linking, relocation and loading of modules compiled using the standard Executable and Linkable Format (ELF).
	This format includes the program code and data, as well as detailed information about unresolved symbols.
	To resolve them, the OS must adjust properly the absolute addresses included in the file, depending on the module's location in memory.
	A relocation type also embedded in the ELF file specifies how the data or code addresses should be updated.
	These types depend on the CPU architecture, for instance, an MSP430 CPU counts with only one type of relocation, while the AVR architecture has 19.
	Both architectures are widely used for embedded systems and IoT devices.
	Once each unresolved symbol is updated with the new address, the \texttt{.data} and \texttt{.bss} sections of the ELF file can be loaded into RAM, and the \texttt{.text} is copied to ROM, then the program can be executed following specific OS functions.
	Overheads of this method include the transmission over the network of a large ELF file depending on the 32 or 64 bits architecture, a symbol table in which names are used to represent each unresolved symbol, also increasing the size of the ELF file, and finally a CPU overhead is incurred while resolving symbols.
	As in the PIC method, no reboot is required to run the new application or to apply an update.
\end{itemize}

Given this three methods of deployment for new applications or feature updates, it is then necessary to evaluate which is the most advisable method 

\section{Component Based Software Engineering (CBSE)}
\label{sec:CBSE}
Several definitions to CBSE have been proposed in the literature, being the notion of \textit{component} the main principle.
In a general way, CBSE aims to leverage the main benefits of SE in terms of development, integration, maintenance, reusability, separation of concerns, among others.
However, a more specific definition was proposed by Szyperski\cite{szyperski2002component}, being one of the most used:
\begin{citeverbatim}
	" A software component is a unit of composition with contractually specified interfaces and explicit context dependencies only. A software component can be deployed independently and is subject to composition by third parties. "
\end{citeverbatim}
In another definition, Heineman \textit{et al.}\cite{heineman2001cbse} define a component as:
\begin{citeverbatim}
	" A component model defines a set of standards for component implementation, naming interoperability, customization, composition, evolution and deployment. "
\end{citeverbatim}
We can note that this definition puts more emphasis in the development model, which not only leverages the abstract system composition, but also covers the global system deployment from the underlying pieces.

The main characteristics of a component can be summarized as follows:
\begin{itemize}
	\item \textbf{Interfaces specification.} The available functionalities of a component.
	\item \textbf{Explicit dependencies.} A component could require other component's functionalities or native libraries to work correctly.
	If so, such requirements should be exposed.
	\item \textbf{Instantiation.} Multiple instances of the given type can exist.
	\item \textbf{Deployment independence.} A deploy unit represent the whole component, which can be reused. However, this feature can be discussed.
\end{itemize}
Moreover, a component-based approach can define an \textit{Architecture Description Language (ADL)}.
This is useful to describe the structure of a software, in a formal way\cite{taylor2009architectural}\cite{len2003software}\cite{medvidovic2000classification}.
ADLs are declarative languages that describe a system's architecture as a set of components, connectors, bindings and configurations.
Such a language can be used to assemble components, based in two elements:

\begin{itemize}
	\item \textbf{Instances.} They are the main elements which actually embody the required application's functionalities, constituting the business logic.
	\item \textbf{Connectors.} A link between component's instances, determined by the exposed provided and required functionalities of the used types.
\end{itemize}

In an ADL, the specified interfaces can be bound between component instances through connectors, which are identified as "required" and "provided" interfaces.
The complexity management can be achieved using a descriptive hierarchical composition, allowing scalability.
For instance, Fractal\cite{bruneton2006fractal} provides a structural description of software architecture.
However, the needs can differ between systems, in order to associate functional, behavioral and system properties with the architecture.

The division in small pieces of a big architecture such as the IoT software layer, can be very useful from a development and maintenance point of view, leveraging the component decoupling and the use of bindings between components to providing services.
Indeed, an ADL can ease the task of composing such layer, giving a complete management of the component's life cycle, from the deployment to the instantiation, being one of the main concerns in IoT systems.
Moreover, we can associate to the defined characteristics of components and its composition an execution environment, which is in charge of the exploitation.
However, the component's implementation is not defined at this level.

\subsection{Service-Oriented Components (SOC)}
As stated in the CBSE definition, decoupling software in pieces defining roles and interfaces reduces complexity, and allows the construction of compatible implementations.
However, in a high-level point of view, constructing applications leads to make a choice between several implementations, and not the needed functionalities.
Thus, a strong coupling exists between code and a given implementation.
In a service-oriented approach, this coupling is avoided delegating the searching and instantiation of a required functionality to a central entity.
Indeed, service-oriented components are the composition between Service-Oriented Architecture (SOA) and component models, which results in high decoupling and increased flexibility.

The general principles of a Service-Oriented Component Model were described by Cervantes \textit{et al.}\cite{cervantes2004autonomous}, seen as a SOA extension to component based development.
These principles are:
\begin{itemize}
	\item \textbf{A service is a provided functionality.} A service is a set of reusable operations.
	\item \textbf{A service is described by a service specification.} It can contain syntactical, behavioral and semantic informations, as well as other specification dependencies.
	\item \textbf{Components implement service specifications.} Being the services the only way of communication between component instances, constraints given by these specifications must be respected.
	\item \textbf{The interaction mechanisms of the service approach are used to solve dependencies between components.} The services, provided by the component instances, are stored in a service registry. This registry is then used to dynamically discover services in order to solve dependencies between them.
	\item \textbf{Compositions are described using services' specifications.} An abstract composition is an specification of services which allows the selection of concrete components. The links are inferred from the services' dependencies.
	\item \textbf{Service specifications provide the basis for substitutability.} A component can be replaced by another having the same specification.
\end{itemize}

Unlike traditional component models, in which we select components before runtime, the interaction principles of the services approach allows to delay this selection until runtime (i.e. from a service provider).
The application will start only if all dependencies of the main component are satisfied.
Another main difference is that the application is defined in a higher abstraction level, in terms of service specifications and not in terms of implementations.
Thus, the result is a Service-Oriented component model facilitating the construction of flexible and dynamic applications.

OSGi and iPOJO, two approaches leveraging the main advantages of the two concepts (loose-coupling and dynamism from the services approach, and a simple development model with a description of the composition from component models) are presented in the next subsections.

\subsection{OSGi}
Closely related to the IoT, OSGi (Open Service Gateway initiative) was originally created to provide a general component model for Java platforms, running on top of domotic residential gateways.
This model aims to implement a dynamic module deployment based in \textit{Bundles}.
The OSGi specification defines a life-cycle manager of this bundles, which are a set of encapsulated components.
A Bundle designates a specific package from a JAR, which is mandatory for the deployment.
Dependency contracts can be declared using manifest files, leveraging the notion of Java packages pointing to other bundles or parts of them.
An \textit{Activator} class represents the internal code of a component, which defines the life-cycle of the Java module including code to start and stop the application.
Moreover, the OSGi framework defines the Java modules that can be deployed at runtime whose granularity dependency is represented using either the JAR or the Java package of the Activator class.
This framework also defines the notion of internal service, using a central services registry already defined in the previous section, allowing dynamic inscription of new services.
Thus, OSGi component contracts are dependency oriented.
Two main implementations of this framework are Apache Felix and Eclipse Equinox, being the first used in several Enterprise Service Bus and the latter the architecture of the Eclipse development environment.

A project called OpenTheBox\footnote{\url{http://openthebox.org/}} is the result of a OSGi platform implementation dedicated to domotics.
It is based on the OSGi platform Knopflerfish\footnote{\url{http://www.knopflerfish.org/}}.
The core of this project relies in a central manager called Apam\cite{damou2013apam} running in the home automation box, which provides an isolated collaboration environment between applications\cite{estublier2012managing} and controls the conflicting accesses to the shared devices\cite{estublier2013resource}.

The specification allows interactions anticipated by the services architecture, but they must be managed manually by the developer.
This task is very delicate and requires a deep knowledge of the OSGi mechanisms in order to finely handle all the possible cases to avoid errors.
A new technology is then necessary to manage automatically all this interactions, proposing a solution to the manual management of them.
This technology, called iPOJO, is described in the next subsection.


\subsection{iPOJO}
Apache iPOJO (injected Plain Old Java Object)\cite{escoffier2007ipojo}\cite{escoffier2007dynamically} is an implementation of the service-oriented components approach.
It was developed at Grenoble Informatics Laboratory (LIG in French), at Ad\`ele team.
It relies in the Apache Felix\footnote{\url{http://felix.apache.org/site/index.html}} OSGi open-source implementation.

As stated in the previous subsection, an easy-to-use approach to develop applications in top of OSGi is required.
In this point of view iPOJO aims to simplify the development of dynamic services applications, implementing service-oriented components already described previously.
This platform is based on the OSGi specification, and consequently, it inherits its characteristics: dynamic, centralized and with Java support only.

The iPOJO technology offers:
\begin{itemize}
	\item \textbf{A model strongly coupled with an execution machine.} In order to give a more comprehensible application structure, all the components of the model exists at execution time.
	\item \textbf{An isolated dynamic service architecture.} Beyond the dynamic service architecture interactions derived from the OSGi platform, an isolation principle is also proposed, providing some services with private characteristics.
	\item \textbf{A simple development model.} The iPOJO execution machine provides introspection injection allowing a transparent management of this concerns to the developer. 
	Thus, the SOA complexity is hidden using the development model from Plain Old Java Object\cite{fowler2000pojo} concealing specially the complexity of the OSGi dynamism.
	\item \textbf{A structural composition language.} iPOJO allows application building from a structured service composition.
	The composition is modeled with the services' specifications, regardless of the implementation.
	This decoupling is one of the iPOJO advantages, since this way the infrastructure choices at runtime an available implementation.
	Therefore, the applications conceived using this method are managed dynamically.
	\item \textbf{Dynamic reconfiguration and introspection functionalities.} This offers a reflected overview of the system structure changes due to dynamism.
	\item \textbf{Extensibility mechanisms.} New non-functional properties can be easily added, such as persistence, security or quality of service.
\end{itemize}

iPOJO leverages this mechanisms to allow a simpler construction of dynamic service applications, while respecting the dynamic services architecture.
An important advantage for the developer is the possibility of manage only the desired non-functional properties.
An application called iCASA\cite{lalanda2014icasa} based on this technologies was created in order to cope with the dynamism of systems such as smart homes.
Being a part of the IoT infrastructure described in this thesis, it is interesting how a service-oriented component architecture can actually manage the dynamic nature of this environments.
However, the highly centralized approach, among with the high specialized tools (even if some simplifications are provided), results in a still complex development model.

A more flexible approach to build component-based applications is discussed in the next section.
Indeed, providing more flexibility in the implementation language, execution environment and architecture definition would lead to a less complex development model.

\subsection{Fractal}
Being a generic, yet extensible, component-based model, Fractal\cite{bruneton2006fractal} is one of the most used component-model approach in the research domain.
Several works\cite{david2005wildcat}\cite{bouchenak2006autonomic}\cite{leclercq2004dream}\cite{romero2010restful} are based on this component model.
Several Fractal implementations exists in different programming languages, providing each one additional properties to the model, specific to a particular domain.
This implementations come from embedded applications with the THINK framework\cite{fassino2002think}, to grid computing using ProActive\cite{caromel2006proactive}.
Other implementations exists for different languages such as Cecilia\cite{cecilia2015} using the C language, Plasma using C++, Julia\cite{bruneton2006fractal} based in Java, FracTalk in SmallTalk and FracNET in the .NET framework.
An API is available to control the dynamic evolution of the application, according to which we can create or destroy instances and bindings.
Even if the hierarchical construction of applications is possible, Fractal does not define a particular mechanism to package and deploy components, since that depends on the chosen implementation.
Thus, we can find in Fractal the classic concepts of a traditional component model (component, interface, connection) previously described.
However, the main difference is that it separates a component into two entities:
\begin{enumerate}
	\item \textbf{Contents.} It contains a finite set of (sub) components and connections.
	\item \textbf{Membrane.} Introspection and manipulation of the components is defined by the membrane, through control interfaces.
	It also defines the connection interfaces to express the required and provided interfaces.
\end{enumerate}

Granted that it offers several implementations in different languages that can be used in the IoT, the lack of deploying mechanisms is a big inconvenient.
Since the goal is to manage a large set of software services using components bound to each other, runtime control on the deployment of new components is mandatory.

%A propos du chapitre 3 :
%Le titre devrait être : Deployment in distributed systems
%L'introduction devrait pas être du background de nouveau, car la transition doit être fait en fin de chapitre 2.
%Personnellement, je changerai l'ordre c'est pas très logique. 
%Je mettrais le CBSE en premier car cela correspond à l'approche traditionnel pour faire du déploiment en système distribué, et je pense que c'est ce qu'il faut dire au début.
%Ensuite tu parles du M@R (en le présentant comme une évolution du CBSE) et à l'intérieur tu peux parler du MDE. Mais le MDE dans l'absolu ce n'est pas important pour toi. etc...


\section{Models@Runtime (M@R)}
With a view to ease software development for these very complex information systems, Model Driven Engineering (MDE) focus on, at first, giving simple, abstract and different points of view of information systems, without modify the actual system. 
In a second place, a variant of the MDE approach, called Model Driven Architecture (MDA)\cite{kleppe2003mda} aims to provide, through Domain Specific Languages (DSLs) coupled with code generators, software development tools and methods.
This approach is able to generate executable code from the abstract model, that can be generated for different hardware architectures.
Moreover, while heterogeneity of target architectures is taken into account through multiple generators, they also imposes a V cycle of development\cite{fouquet2013kevoree}, which consists in:
\begin{itemize}
	\item Context design of the abstract system ("Meta-model" and code generators)
	\item Use-case design (model)
	\item Executable code from the model
\end{itemize}
Even if this V cycle responds to the large complexity, it could not be enough to respond in terms of software plasticity that will be present in the very changing IoT environment.

Classical information systems are nowadays a critical piece of many important processes as well as industrial than commercial.
Downtime of this systems can represent a big economic loss, therefore robustness and continuous availability should be assured.
Thus, we can treat them as \textit{eternal systems}.
Furthermore, the evolution of the needs and the rapid obsolescence of features imposes an openness to new functionalities that were not predictable at design time.
In IoT systems, we can take the example given in subsection \ref{subsec:effBuilding}, in which evolutions of a building were complicated to predict.
It was even more difficult to provide a prepared software platform able to deal with new features and devices, avoiding the rapid availability of new services.
However, this new behavior must be implemented in the future due to the new energy policies, without alter the current given services.

We can consider this critical systems as \textit{Dynamic Adaptive Systems} (DAS)\cite{mckinley2004composing}, \cite{morin2009taming}.
Continuous update mechanisms are then carried into the target platforms, in order to change at runtime the software already deployed.
Taking into account this dynamic behavior, DAS are defined using a paradigm based on components, being the management of these components' deployment an evolution of the approach. %which allows to decouple this big systems into software pieces that are easily maintainable\cite{crnkovic2002building}.

DAS were typically deployed in critical platforms such as airports or banks, which had no tolerance to downtimes.
However, in the last years the pervasiveness of software in all domains demand a downtime rate near to 0, including phones, domestic Internet gateways and domotic services\cite{nain2008using}.
This requirements need to be supported by continuous updates, even for this non-critical systems.
Even if today IoT systems are not widely deployed, its importance in the near future justify its design in the form of a DAS.

The development model was also changed drastically, in order to follow the software plasticity present in this systems.
When V cycle was widely used and preferred over other development methods in the 80's, coming from the design and initial specification to the code generation or implementation, nowadays Agile methods\cite{stolberg2009enabling}, which aim to bring in shorter development cycles, are more recommended and used, in order to respond more quickly to specification evolutions thanks to users feedback.
Combined to this, the new approach of Continuous Integration (CI) introduces a new test system able to be updated with new artifacts of continuous development.
By adopting such a methodology, non-critical systems and DAS specified constraints are brought more closely, supporting the idea of move together abstractions of this two domains.

Therefore, the V approach from the 80's is becoming obsolete, while agile methods tend to expand it at every development cycle.
It is then possible to develop and deploy software for critical and non-critical systems in a continuous manner.
MDE approaches to generate code, and especially the MDA unidirectional approach which leverages a model to produce code, must take into account the inherent bidirectional development model of this continuous cycle.
Moreover, generators must provide reverse operations to allow cyclic code, which can be present at design time.
This can be useful to deal with legacy code as much as at design time, which is one of the main concerns of MDE, as in the tooling, beyond an approach of design to code.

\subsection{M@R in Dynamic Adaptive Systems}
A new paradigm named \textit{Model@Runtime} (M@R) aim to combine MDE techniques with tangible systems, in order to respond to the problem of cyclic design stated previously.
As MDE, M@R were useful at first as a thoughtful visualization of a system, for simulation purposes\cite{oreizy1999architecture}, \cite{blair2009models}, \cite{zhang2006model}.
A \textit{permanent updated} model is then used to represent abstractly a DAS at runtime.
Every different element composing this model can be represented in a schema, providing an easy navigation and an introspective analysis through the model layer.
Reasoning about the state of the system is also possible using the same layer.
Leveraging not only the introspection capability of the model, but also the intersection, Brice Morin\cite{morin2010leveraging} worked with this Model@Runtime layer aiming to modify it, through the reflexive model representation.
Using intersection allows to modify the internal state of a system\cite{paepcke1993object}.
Morin's M@R approach aim to build systems with reflexive capacities, having also intersection and introspection features present in the same layer.
As an abstract layer, the proposed reflexivity can be asynchronous, allowing modifications before affecting the real system, i.e. for testing purposes. 

This asynchronous properties separate strongly the actual system and the model, giving to MDE techniques the possibility to manipulate the reflexive layer without affecting at all the running platform.
Moreover, a schematic representation leveraging the reflexive layer can be extracted from the actual system, in order to modify it.
This modification can be in the form of component's adding or removal in the extracted model, then thanks to a version comparison between both the actual and the modified model, it is possible to actually trigger updates on the platform.
A bidirectional connexion also exists, since any modification to the external platform will be reflected in the M@R layer.
Modifications in the model can be manipulated before the deployment allowing verifications, in addition to a more flexible way to test different configurations.
For instance, if we want to deploy components with dependencies, the platform itself will avoid the deployment if any of the dependency is not met.
At the model level, the same changes can be executed regardless of the order, if the adaptation execution is done after the adding/removal of the components, being the model less constrained than the platform.
While the restrictions of the platform avoid the direct use of MDE approaches to manage the adaptations, the model can be manipulated to delay this restrictions in the application of the reflexive model, allowing to MDE approaches manipulate the model without following any order.

Approaches such as feature models or aspects\cite{morin2009taming} coupled with composition algorithms, can leverage the asynchronous capabilities of M@R in order to compose a model from the DAS architecture.
Since all operations can be done \textit{offline}, no constraints are imposed by the tangible platform before the deployment, while the system can decide when to synchronize.
Conceiving and composing models is then essential to assemble a whole DAS.
Several paradigms of composition are also needed, based in previous works\cite{morin2009mar},\cite{ko2012low},\cite{rouvoy2009music}, that encourage the use of software components to encapsulate the life cycle and composition operators.
Moreover, this paradigms are also needed to explore the exploitation viability at this granularity level, in order to manage the different parts of DAS application layer. 

\subsection{A DAS based on the IoT: Challenges}
Given the complexity and the vast heterogeneity of an IoT architecture, this thesis proposes the use of a M@R approach to manage the life cycle of the software deployed in this systems.
The approach will follow the directions given by the research already done by Fran{\c{c}}ois Fouquet\cite{fouquet2013kevoree}, previously based on Brice Morin's thesis\cite{morin2010leveraging}.
The DAS representation of the IoT will result, as in the mentioned thesis, in a Dynamic DAS (DDAS), in which we found a very different behavior than in traditional DDAS, such as Data Centers or Cloud Computing.
The challenges raised by such an approach in an IoT environment are, first of all, linked to the constrained environment already described in section \ref{subsec:constrainedObj}.
As the current implementations of the M@R paradigm are done in high level languages, one main challenge is to adapt such implementations to development environments for embedded systems in which we cannot make use of such languages, resulting in a difficult development task.
Moreover, the decoupling of components in modern operating systems is facilitated by Inter Process Communication (IPC) APIs already provided.
In the OS described in section \ref{subsec:IoTOS}, and more particularly in Contiki, which is used in this thesis, IPC are less evident nor standard.
Thus, a new way to implement components must be proposed following the directions discussed in section \ref{sec:CBSE}.
A second challenge resides in the network topology typical of IoT.
Being a multi-hop communication the mainly used approach to reach nodes in the IoT, different abstractions are needed to take it into account.
Furthermore, the network traffic must be reduced to a minimum, since wireless devices use batteries to work, and being communication the most energy consuming task of an IoT node.


%An IoT system is very complex and should be implemented as a DAS.
%This is important because the management of IoT infrastructures is impossible if the current approaches still being used, since each subsystem is treated as a separated entity of the whole system.
%The limitations given by the hardware of IoT devices must be taken into account, while providing the means to reach an abstraction level typical of DAS design.
%The very hard task to decouple an embedded application into small pieces should be coped using a CBSE approach, since works converge in the use of this method for DAS

%\subsection{Special distributed systems: Wireless Sensor Networks}

\section{Kevoree}
The Kevoree\footnote{\url{http://www.kevoree.org}} project, was created as a part of Fran{\c{c}}ois Fouquet's PhD thesis.
A part of the contributions of this thesis were based on this project, being the most important the implementation of the Kevoree meta-model in C language and the adaptation of an essential part of the framework as a Contiki application for the IoT.
This section presents the most important concepts that were essential to the achievement of this thesis.
This concepts are thus very important to the general understanding of the next chapters.

Kevoree is a component-based development framework for applications running on DAS, based on the paradigm of Models@Runtime.
This approach proposes an abstract model through which it is possible to manipulate the different concepts that characterizes a distributed system.

\subsection{Main Kevoree features}
Kevoree aims to provide an abstraction able to manipulate the main concepts of a distributed system, in order to ease the adaptations management for this system.
To do that, Kevoree proposes several features: synchronization and de-synchronization between the reflexive model and the actual system at runtime, separation of concerns between the business logic and its interactions and finally the dissemination of reconfigurations and resource heterogeneity on which the system is being executed.
\begin{itemize}
	\item \textbf{Separation of concerns.} A distributed application is composed of specific business logic but also communication means (code).
	Unlike business logic, communication means does not have necessarily specific code due to the application.
	Thus, it results interesting to separate these two entities, which allows to reuse the different software blocks.
	This simplifies the component's business logic development, since the communication concerns are separated thus hidden.
	Moreover, the adaptation of the communication means between components regarding the context is required for a distributed application.
	\item \textbf{Distribution management.} In order to distribute different functionalities to the different nodes in a system, an abstract representation is provided, in which this characteristics are modeled and can be manipulated at runtime.
	\item \textbf{De-synchronization.} A process of validation carried in the reflexive model before the application is one of the main advantages of this feature.
	This de-synchronization is possible thanks to the concept of models@runtime that is the base of Kevoree, in which an adaptation is defined through a model that can be validated.
	Coherence of the configurations is then validated to be sure that an unstable behavior can be reached or a complete breakdown can happen.
	In a distributed context, this is of high importance, in order to avoid an adaptation that cannot be executed by all nodes.
	\item \textbf{Adaptations dissemination.} Once an adaptation is executed, every node must be notified of this change, so it can be taken into account by the whole system.
	However, in a distributed system, and specially in an IoT system, it is not possible to guarantee an uninterrupted communication between nodes, since IoT networks are often a subject of communication errors or disconnections.
	This constraints are considered in the dissemination of the adaptations, to provide a coherent evolution.
	Different synchronization methods are then used regarding the communication means between nodes.
	\item \textbf{Execution platforms heterogeneity.} Distributed systems are composed of several execution platforms, such as mobile nodes (smartphones), PC, servers or embedded systems. In the particular case of the IoT all this kind of systems are involved, since the tasks are distributed according to the hardware capacities of the available platforms.
	It is then necessary to represent in the Kevoree model the differences between participants and their specific characteristics.
\end{itemize}

\subsection{Kevoree internals}

\section{Context}
\label{sec:Context}

\subsection{IoT and Models@runtime}
bla

\subsection{Applications}
bla

\subsection{The problem}
bla

\subsection{The approach}
bla

\section{Related work}
\label{sec:relatedWork}

\subsection{Dynamic reconfiguration in Wireless Sensor Networks}

\subsection{Dynamic adaptation}

\subsection{Firmware update managers}

\subsection{Models@runtime}